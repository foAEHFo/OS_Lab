<h1 align="center"> lab3：断,都可以断 </h1>

 <div align="center">

张德民 刘越帅 欧广元

</div>

## 实验目的

1. 了解第一个用户进程创建过程
2. 了解系统调用框架的实现机制
3. 了解ucore如何实现系统调用sys_fork/sys_exec/sys_exit/sys_wait来进行进程管理

## 实验内容
实验4完成了内核线程，但到目前为止，所有的运行都在内核态执行。实验5将创建用户进程，让用户进程在用户态执行，且在需要ucore支持时，可通过系统调用来让ucore提供服务。为此需要构造出第一个用户进程，并通过系统调用sys_fork/sys_exec/sys_exit/sys_wait来支持运行不同的应用程序，完成对用户进程的执行过程的基本管理。

## 实验过程

### 练习0 填写已有实验

本实验依赖实验2/3/4。请把你做的实验2/3/4的代码填入本实验中代码中有“LAB2”/“LAB3”/“LAB4”的注释相应部分。注意：为了能够正确执行lab5的测试应用程序，可能需对已完成的实验2/3/4的代码进行进一步改进。

### 练习1: 加载应用程序并执行（需要编码）

do_execv函数调用load_icode（位于kern/process/proc.c中）来加载并解析一个处于内存中的ELF执行文件格式的应用程序。你需要补充load_icode的第6步，建立相应的用户内存空间来放置应用程序的代码段、数据段等，且要设置好proc_struct结构中的成员变量trapframe中的内容，确保在执行此进程后，能够从应用程序设定的起始执行地址开始执行。需设置正确的trapframe内容。

#### 请在实验报告中简要说明你的设计实现过程。

    /* 将用户栈指针设置为用户栈顶（USTACKTOP） */
        tf->gpr.sp = USTACKTOP;

        /* 从 ELF 头获取程序入口地址并设置 sepc/epc */
        tf->epc = elf->e_entry;

        /* 构造返回到用户态时的 sstatus：
        * - 清除 SPP，使 sret 返回到用户态（U-mode）；
        * - 置位 SPIE，使 sret 后中断按 SPIE 的值恢复；
        * - 清除 SIE（在内核中禁用中断，等待 sret 由 SPIE 恢复）；
        * 同时尽量保留原有 sstatus 的其它位（如 SUM），以免破坏访问策略。
        */
        uintptr_t newsstatus = (sstatus & ~SSTATUS_SPP) | SSTATUS_SPIE;
        newsstatus &= ~SSTATUS_SIE;
        tf->status = newsstatus;

如上边的代码，就是我补充的第六步，主要是按照注释进行编写代码。

首先tf->gpr.sp是用户栈指针，应该设置为用户栈顶 所以设置为USTACKTOP，tf->epc是程序入口地址，所以设置为 ELF 头中的 e_entry。

tf->status是进程状态寄存器sstatus，应该设置为适合用户程序运行的值，首先拿用户态原本的 sstatus，然后把SSTATUS_SPP置零，说明这个进程应该去往用户态运行，以后执行sret时，返回到用户态。

然后把SSTATUS_SPIE置1，sret后用户态恢复“中断开启”。

最后SSTATUS_SIE置为0，表明在内核态不希望发生中断。

#### 请简要描述这个用户态进程被ucore选择占用CPU执行（RUNNING态）到具体执行应用程序第一条指令的整个经过。

kern_init 执行完 proc_init 后，会调用 cpu_idle()。在 cpu_idle() 中，有一个 while (1) 循环，由于 idleproc->need_resched = 1，循环会立即调用 schedule()。schedule() 会从就绪队列中选择下一个进程（也就是我们的用户进程user_main），然后通过 proc_run() 切换到它。此时，current变为user_proc，开始执行user_main函数。

在proc_run()中，首先禁用中断，保存当前进程指针，然后切换页表，切换上下文，最后打开中断，但是切换完成后，CPU在新进程的上下文中执行，但仍在内核态。

新进程恢复后，从forkret函数开始执行（这是switch_to恢复的返回地址，由copy_thread设置）。

forkret 调用 forkrets(current->tf)。

    .globl forkrets
    forkrets:
        # set stack to this new process's trapframe
        move sp, a0
        j __trapret


这是一个汇编函数，将栈指针sp设置为当前进程的 trapframe 地址（move sp, a0），然后跳转到 __trapret。

    .globl __trapret
    __trapret:
        RESTORE_ALL
        # return from supervisor call
        sret
 

__trapret 调用 RESTORE_ALL 宏，从 trapframe 中恢复所有寄存器，包括通用寄存器 (x0-x31)、状态寄存器 (sstatus)、异常程序计数器 (sepc/epc) 等。

user_main函数中，先声明链接时嵌入的用户程序二进制数据的起始地址和大小，然后调用kernel_execve函数，kernel_execve函数参数里a0存系统调用号，a1到a5是参数。然后调用一个ebreak中断，进入中断处理机制。然后保存当前的陷阱帧，调用sys_exec，sys_exec调用 do_execve，执行程序替换，清除旧内存，调用 load_icode 加载用户程序并设置新的用户trapframe等(也就是设置用户栈，epc设置为用户程序开始的地方，更新status寄存器等等)。

然后执行__trapret，先调用RESTORE_ALL宏，从新的用户trapframe恢复所有寄存器（包括 sepc、sstatus 等）。然后执行sret指令，sret 根据 sepc 设置 PC，并根据 sstatus 切换特权级别，返回到第一条用户指令去执行。

### 练习2: 父进程复制自己的内存空间给子进程（需要编码）

创建子进程的函数do_fork在执行中将拷贝当前进程（即父进程）的用户内存地址空间中的合法内容到新进程中（子进程），完成内存资源的复制。具体是通过copy_range函数（位于kern/mm/pmm.c中）实现的，请补充copy_range的实现，确保能够正确执行。

#### 请在实验报告中简要说明你的设计实现过程。

我实现的代码如下：


            /* (1) 找到 src_kvaddr：页（page）对应的内核虚拟地址
            * (2) 找到 dst_kvaddr：新页（npage）对应的内核虚拟地址
            * (3) 将 src_kvaddr 指向的内存内容拷贝到 dst_kvaddr，拷贝长度为 PGSIZE（页大小）
            * (4) 建立新页（npage）的物理地址与起始线性地址（start）之间的映射关系
             */
              /* (1) 源页面的内核虚拟地址 */
            void *src_kvaddr = page2kva(page);
            /* (2) 目标页面的内核虚拟地址 */
            void *dst_kvaddr = page2kva(npage);

            /* (3) 拷贝整页内容 */
            memcpy(dst_kvaddr, src_kvaddr, PGSIZE);

            /* (4) 在目标页表中建立映射，权限与源页相同 */
            ret = page_insert(to, npage, start, perm);
            if (ret != 0)
            {
                /* 建立映射失败，释放分配的页 */
                free_page(npage);
                return -E_NO_MEM;
            }
            /* 记录该页的虚拟地址映射信息（与 pgdir_alloc_page 一致） */
            npage->pra_vaddr = start;

首先，使用page2kva函数，找到父进程的页的对应内核虚拟地址，然后再用page2kva得到要为子进程分配的页的内核虚拟地址，然后把父进程的数据直接全部拷贝到子进程的页里。最后，在子进程的页目录里建立虚拟地址 start 到物理页面 npage 的映射，并记录这个页的虚拟地址信息。

#### 如何设计实现Copy on Write机制？给出概要设计，鼓励给出详细设计。

    Copy-on-write（简称COW）的基本概念是指如果有多个使用者对一个资源A（比如内存块）进行读操作，则每个使用者只需获得一个指向同一个资源A的指针，就可以该资源了。若某使用者需要对这个资源A进行写操作，系统会对该资源进行拷贝操作，从而使得该“写操作”使用者获得一个该资源A的“私有”拷贝—资源B，可对资源B进行写操作。该“写操作”使用者对资源B的改变对于其他的使用者而言是不可见的，因为其他使用者看到的还是资源A。

##### 概要设计

1. 核心原理：
   - 共享阶段：在fork时，父子进程共享父进程的物理页面，不复制内容。页面映射为只读（去掉写权限PTE_W），防止写操作破坏共享。
   - 写时复制：当某个进程尝试写共享页面时，触发page fault。内核检测到是COW页面（只读但原页面可写），复制页面内容到新物理页面，更新映射为可写。
   - 隔离保证：写操作后，每个进程拥有私有拷贝，修改不可见。

2. 关键组件：
   - 页面管理：利用`Page`结构体的`ref`字段（引用计数）跟踪共享页面。当`ref > 1`时，表示页面被共享。
   - 权限控制：在页表项(PTE)中设置只读权限（去掉PTE_W）。
   - 异常处理：扩展trap处理（`kern/trap/trap.c`），在page fault时检查是否为COW写操作。
   - 内存一致性：确保复制时不影响其他进程。


##### 详细设计

1. **修改copy_range函数（kern/mm/pmm.c）**：
   - 当前实现是直接复制：分配新页面`npage`，`memcpy`内容，然后`page_insert`。
   - **COW版本**：改为共享父进程页面，不复制内容。
     - 获取父进程页面`page`后，不分配`npage`，而是直接使用`page`。
     - 设置权限为只读：`perm &= ~PTE_W;`（去掉写权限）。
     - `page_insert(to, page, start, perm);` 将父页面映射到子进程页表（只读）。
     - 增加引用计数：`page_ref_inc(page);`。
     - 移除`memcpy`和`npage`分配逻辑。

   - **代码片段**（替换现有的LAB5:EXERCISE2部分）：
     ```c
     if (*ptep & PTE_V) {
         uint32_t perm = (*ptep & PTE_USER);
         struct Page *page = pte2page(*ptep);
         // COW: 设置为只读权限，共享页面
         perm &= ~PTE_W;
         int ret = page_insert(to, page, start, perm);
         if (ret != 0) {
             return -E_NO_MEM;
         }
         // 增加引用计数，表示共享
         page_ref_inc(page);

     }
     ```
   - **注意**：`npage->pra_vaddr = start;` 可移除，因为共享页面时虚拟地址由各进程页表决定。

2. 扩展Page Fault处理（kern/trap/trap.c）：

   - 添加COW逻辑：在`trap_dispatch`中，检测page fault（异常码为load/store page fault）。
     - 检查fault地址对应的PTE：如果页面只读（无PTE_W）且ref > 1，则触发COW复制。
     - **复制步骤**：
       - 分配新页面`npage`。
       - `memcpy`原页面内容到`npage`。
       - 更新当前进程的页表：`page_insert`新页面为可写（perm | PTE_W）。
       - 减少原页面的ref：`page_ref_dec(page);` 如果ref==0，释放页面。

     - **代码片段**（在trap_dispatch中添加）：
       ```c
       case CAUSE_LOAD_PAGE_FAULT:  // 或STORE_PAGE_FAULT，根据RISC-V定义
       case CAUSE_STORE_PAGE_FAULT:
           uintptr_t addr = tf->badvaddr;  // fault地址
           pte_t *ptep = get_pte(current->pgdir, addr, 0);
           if (ptep && (*ptep & PTE_V) && !(*ptep & PTE_W)) {
               struct Page *page = pte2page(*ptep);
               if (PageCOW(page)) {  // 共享页面
                   struct Page *npage = alloc_page();
                   if (!npage) return -E_NO_MEM;
                   void *src_kvaddr = page2kva(page);
                   void *dst_kvaddr = page2kva(npage);
                   memcpy(dst_kvaddr, src_kvaddr, PGSIZE);
                   uint32_t perm = (*ptep & PTE_USER) | PTE_W;  // 设为可写
                   page_insert(current->pgdir, npage, addr, perm);
                   page_ref_dec(page);  // 减少共享计数
                   npage->pra_vaddr = addr;  // 记录新页面的虚拟地址
                   return 0;  // 继续执行
               }
           }
           break;
       ```

3. **其他修改**：
   - **权限一致性**：在fork时，确保父进程页面也设为只读（修改`do_fork`或`copy_thread`中父进程的权限）。
   - **退出处理**：在`do_exit`中，正确减少ref计数，避免内存泄漏。

4. **状态转换（有限状态自动机风格）**：
   - **初始状态**：页面未共享，ref=1，可写。
   - **共享状态**：fork后，ref>1，只读（COW标记）。
   - **写触发**：page fault → 复制 → 新页面ref=1，可写；原页面ref--。
   - **释放**：进程退出时，ref--；若ref=0，释放页面。






### 练习3: 阅读分析源代码，理解进程执行 fork/exec/wait/exit 的实现，以及系统调用的实现（不需要编码）

请在实验报告中简要说明你对 fork/exec/wait/exit函数的分析。并回答如下问题：

请分析fork/exec/wait/exit的执行流程。重点关注哪些操作是在用户态完成，哪些是在内核态完成？内核态与用户态程序是如何交错执行的？内核态执行结果是如何返回给用户程序的？

请给出ucore中一个用户态进程的执行状态生命周期图（包执行状态，执行状态之间的变换关系，以及产生变换的事件或函数调用）。（字符方式画即可）

执行：make grade。如果所显示的应用程序检测都输出ok，则基本正确。（使用的是qemu-1.0.1）

### 扩展练习 Challenge

1. 实现 Copy on Write （COW）机制

给出实现源码,测试用例和设计报告（包括在cow情况下的各种状态转换（类似有限状态自动机）的说明）。

这个扩展练习涉及到本实验和上一个实验“虚拟内存管理”。在ucore操作系统中，当一个用户父进程创建自己的子进程时，父进程会把其申请的用户空间设置为只读，子进程可共享父进程占用的用户内存空间中的页面（这就是一个共享的资源）。当其中任何一个进程修改此用户内存空间中的某页面时，ucore会通过page fault异常获知该操作，并完成拷贝内存页面，使得两个进程都有各自的内存页面。这样一个进程所做的修改不会被另外一个进程可见了。请在ucore中实现这样的COW机制。

由于COW实现比较复杂，容易引入bug，请参考 https://dirtycow.ninja/ 看看能否在ucore的COW实现中模拟这个错误和解决方案。需要有解释。

这是一个big challenge.

#### 实现源码

这一部分我将会介绍我的实现逻辑，主要是修改了pmm.c和trap.c里边的相应逻辑。

首先是pmm.c中的copy_range函数。

    int copy_range(pde_t *to, pde_t *from, uintptr_t start, uintptr_t end,
               bool share)
    {


        assert(start % PGSIZE == 0 && end % PGSIZE == 0);//确保页对齐
        assert(USER_ACCESS(start, end));//确保用户空间访问
        // 通过页为单位复制内容
        do
        {
            // call get_pte to find process A's pte according to the addr start
            pte_t *ptep = get_pte(from, start, 0), *nptep;//获取源进程的页表项，nptep用于存储目标进程的页表项
            
            if (ptep == NULL)
            {
                start = ROUNDDOWN(start + PTSIZE, PTSIZE);
                continue;
            }

            // call get_pte to find process B's pte according to the addr start. If
            // pte is NULL, just alloc a PT
            if (*ptep & PTE_V)//如果源页表项有效并且存在映射
            {

                if ((nptep = get_pte(to, start, 1)) == NULL)//为目标进程分配页表项
                {
                    
                    return -E_NO_MEM;
                }
                

                uint32_t perm = (*ptep & PTE_USER);
                // get page from ptep
                struct Page *page = pte2page(*ptep);//得到源进程的物理页
                

                // alloc a page for process B
                //struct Page *npage = alloc_page();
                assert(page != NULL);
                //assert(npage != NULL);
                int ret = 0;

                
                *ptep = pte_create(page2ppn(page), perm & ~PTE_W);
                tlb_invalidate(from, start);  // 刷新父进程的TLB

                // 确保子进程的页表项是干净的，避免引用计数错误
                page_remove_pte(to, start, nptep);

                ret = page_insert(to, page, start, perm & ~PTE_W);
                tlb_invalidate(to, start);


                if (ret != 0)
                {
                    
                    /* 建立映射失败，释放分配的页 */
                    //free_page(npage);
                    return -E_NO_MEM;
                }

                
                //npage->pra_vaddr = start;
                assert(ret == 0);
            }

            start += PGSIZE;
        } while (start != 0 && start < end);

        
        return 0;
    }

这一段是当父进程创建子进程时候使用的，之前我们使用的方法是直接为子进程复制一个一模一样的页，供子进程使用。

COW机制则不同，我们首先获得父进程的物理页，然后把子进程的虚拟地址也映射到这个页，同时去除写权限。需要注意的是，同时也要检查父进程的权限，父进程的写权限也要去除。

这样就完成了之前复制的任务，实现了子进程和父进程共享资源，可以一起读，但是当出现写的情况的时候，就会触发CAUSE_STORE_PAGE_FAULT的异常，然后就去trap.c里去进行处理。

    case CAUSE_STORE_PAGE_FAULT:
        // COW: Store page fault通常是COW触发，设置COW标记在安全上下文处理

        uintptr_t addr = ROUNDDOWN(tf->tval, PGSIZE);  // fault页对齐地址，tval里存放异常地址
        pde_t *pgdir = (pde_t *)current->mm->pgdir;
        // 检查pgdir是否有效
        if (!pgdir) {
            cprintf("ERROR: pgdir is NULL\n");
            current->flags |= PF_EXITING;
            break;
        }

        
        pte_t *ptep = get_pte(pgdir, addr, 0);
        if (ptep && (*ptep & PTE_V) && !(*ptep & PTE_W)) {//如果页有效且不可写，标记COW
            struct Page *page = pte2page(*ptep);//获取物理页
            if (page&&page_ref(page)>1) {
                struct Page *npage = alloc_page();//分配新页
                if (!npage) {
                        current->flags |= PF_EXITING;
                        break;
                    }
                
                void *src_kvaddr = page2kva(page);
                void *dst_kvaddr = page2kva(npage);
                
                memcpy(dst_kvaddr, src_kvaddr, PGSIZE);//复制

                uint32_t perm = (*ptep & PTE_USER) | PTE_W;  // 设为可写
                
                page_remove(pgdir, addr);
                
                if (page_insert(pgdir, npage, addr, perm) != 0) {
                         free_page(npage);
                         current->flags |= PF_EXITING;
                         break;
                     }
                cprintf("page num%d\n",page_ref(page));
                npage->pra_vaddr = addr;
                tlb_invalidate(pgdir, addr);  // 刷新TLB
                return;
            } 
        
        else{
            if (page) {
                    uint32_t perm = (*ptep & PTE_USER) | PTE_W;
                    *ptep = pte_create(page2ppn(page), perm);
                    tlb_invalidate(pgdir, addr);
                    return;
                }
            }
        }
        break;


上边就是对于存储页异常的处理，首先我们从tf中获取出现异常的地址，然后使用ROUNDDOWN实现向下对齐，因为处理页的时候必须对齐处理。

然后我们读取这个页的页表项，进行检查，看看它是不是有效且不可写，这样就符合了COW的要求，继续向下获取物理页，然后检查引用次数是否大于1。

如果大于1，那么就按照COW机制进行处理，处理方法和之前是一样的，即新分配一个页，然后把父进程的信息全部复制上去，然后在子进程的页表里修改这个页表项和页的映射关系即可。注意这时候就具有了写的权限。

当引用次数等于1的时候，我们就没有必要再复制了，直接修改一下写权限就行了，后续如果创建了子进程的话，我们也进行了处理，重新把它设置为不可写的状态。

综上就实现了COW机制。

#### 设计报告（包括COW情况下的各种状态转换说明）

##### COW机制有限状态自动机设计

COW机制可以用有限状态自动机来描述页面的生命周期和状态转换。以下是详细的状态转换设计：

**状态定义：**

1. **私有可写状态**
   - 引用计数 = 1
   - 写权限 = 1（可写）
   - 页面被单一进程独占，可以自由读写

2. **共享只读状态**
   - 引用计数 > 1
   - 写权限 = 0（只读）
   - 页面被多个进程共享，处于COW保护中

3. **私有只读状态**
   - 引用计数 = 1
   - 写权限 = 0（只读）
   - 其他进程已退出，但页面仍为只读状态

4. **页面错误待处理状态**
   - 写操作触发页面错误
   - 临时状态，等待COW处理

5. **页面复制中状态**
   - 正在分配新页面
   - 正在复制页面内容
   - 临时状态

**状态转换图：**

如下图所示。

<p align="center">
  <img src="figure/zdmfig2.png" alt="figure">
</p>

**状态转换详细说明：**

**转换1：私有可写 → 共享只读**
- **触发事件**：执行fork()系统调用
- **执行条件**：父进程创建子进程
- **状态变化**：引用计数 1→2，写权限 1→0

**转换2：共享只读 → 页面错误待处理**
- **触发事件**：任一进程尝试写入共享页面
- **执行条件**：写操作 && 写权限=0 && 引用计数>1
- **具体操作**：触发CAUSE_STORE_PAGE_FAULT异常

**转换3：共享只读 → 私有不可写**
- **触发事件**：其他共享进程退出
- **执行条件**：引用计数减少到1，但写权限仍为0
- **状态变化**：引用计数 >1→1，写权限保持0

**转换4：页面错误待处理 → 页面复制中**
- **触发事件**：页面错误处理程序识别为COW
- **执行条件**：页表项有效 && 不可写 && 引用计数>1

**转换4a：页面错误待处理 → 私有可写（直接权限恢复）**
- **触发事件**：页面错误处理程序检测到私有页面
- **执行条件**：页表项有效 && 不可写 && 引用计数=1
- **状态变化**：引用计数保持1，写权限 0→1

**转换5：私有只读 → 页面错误待处理**
- **触发事件**：进程尝试写入私有的只读页面
- **执行条件**：写操作 && 写权限=0 && 引用计数=1
- **具体操作**：触发CAUSE_STORE_PAGE_FAULT异常

**转换6：页面复制中 → 私有可写**
- **触发事件**：COW复制完成
- **执行条件**：新页面分配成功 && 内容复制完成
- **状态变化**：创建新的私有页面，引用计数=1，写权限=1


**并发安全性考虑：**

1. **原子性保证**：
   - 页表项修改使用原子操作
   - TLB刷新确保缓存一致性
   - 引用计数操作受到内存管理锁保护

2. **竞争条件处理**：
   - 多个进程同时写入：第一个触发COW，其他进程等待
   - 页面释放竞争：引用计数为0时才真正释放
   - 进程退出竞争：正确处理引用计数减少

3. **错误处理**：
   - 内存分配失败：设置进程退出标志
   - 页面插入失败：释放已分配页面，返回错误

**性能优化策略：**

1. **延迟复制**：只有真正写入时才复制页面
2. **权限优化**：引用计数=1时直接恢复写权限
3. **TLB管理**：精确刷新相关页面的TLB条目
4. **内存回收**：及时释放不再使用的页面

**状态机设计优势：**

1. **逻辑清晰**：每个状态的职责和转换条件明确
2. **易于维护**：状态转换逻辑集中管理
3. **调试友好**：便于跟踪页面状态变化和问题定位
4. **扩展性强**：易于添加新的状态和转换逻辑
5. **安全性高**：明确的状态约束防止非法操作

#### 测试用例

以下是我的测试用例

    #include <ulib.h>
    #include <stdio.h>

    int num = 0;

    int
    main(void) {
        int pid;

        cprintf("Parent: initial num = %d\n", num);

        pid = fork();
        assert(pid >= 0);

        if (pid == 0) {
            // 子进程
            cprintf("Child: before write, num = %d\n", num);
            num = 100;
            cprintf("Child: after write, num = %d\n", num);
            exit(0);
        }

        wait();  

        cprintf("Parent: after child exit, num = %d\n", num);

        num=1895;

        cprintf("Father: after write, num = %d\n", num);

        pid = fork();
        assert(pid >= 0);

        if (pid == 0) {
            // 子进程
            cprintf("Child: before write, num = %d\n", num);
            num = 1000;
            cprintf("Child: after write, num = %d\n", num);
            exit(0);
        }

        wait();  

        cprintf("Parent: after child exit, num = %d\n", num);


        cprintf("COW test pass.\n");
        return 0;
    }

得到的结果如下，可以看见，最初的num值为0，子进程修改后变成了100，但是父进程的值还是0。

然后子进程结束，父进程把值修改为1895，新的子进程获得的num值就是1895，修改后变为1000，但是父进程中的值还是1895。

综上，测试成功。

<p align="center">
  <img src="figure/zdmfigure1.png" alt="figure">
</p>


2. 说明该用户程序是何时被预先加载到内存中的？与我们常用操作系统的加载有何区别，原因是什么？





## 列出你认为本实验中重要的知识点，以及与对应的OS原理中的知识点，并简要说明你对二者的含义，关系，差异等方面的理解（也可能出现实验中的知识点没有对应的原理知识点）






## lab2分支任务：gdb 调试页表查询过程
本实验核心目标：理解虚拟地址到物理地址的转换过程：通过双重GDB调试，观察CPU访问虚拟地址时，QEMU如何模拟硬件完成地址翻译。
在QEMU中，GDB调试功能主要通过以下关键组件和调用路径实现：

(1)GDB Stub初始化
gdbserver_start() 初始化GDB服务器
gdb_handlesig() 处理来自guest的信号
gdb_read_byte() 读取GDB命令

(2)内存访问相关调用路径
target_memory_rw_debug() 提供调试内存读写接口
cpu_memory_rw_debug() CPU特定的内存读写实现
对于物理内存模式，直接使用 cpu_physical_memory_read() / cpu_physical_memory_write()

(3)地址翻译调用路径
riscv_cpu_get_phys_page_debug() 获取虚拟地址对应的物理地址
get_physical_address() 执行实际的页表遍历和地址翻译     


在地址翻译过程中，有几个关键的分支判断：
特权级别判断：

```c
if (mode == PRV_M) {
    ...
} else {
    ...
}
```
页表项有效性检查：

```c
if (!(pte & PTE_V)) {
    /* Invalid PTE */
    return TRANSLATE_FAIL;
}
```

权限检查：
```c
if (access_type == MMU_DATA_LOAD && !((pte & PTE_R) ||
           ((pte & PTE_X) && mxr))) {
    /* Read access check failed */
    return TRANSLATE_FAIL;
}
```
下面是具体调试：
1. 编译带调试信息的qemu:
```
# 进入QEMU源码目录
cd qemu-4.1.1

# 清理之前的编译结果
make distclean

# 重新配置，这次要带上调试选项
./configure --target-list=riscv32-softmmu,riscv64-softmmu --enable-debug

# 重新编译
make -j$(nproc)
```
![alt text](image1.png)
2. 修改Makefile指向新编译的调试版QEMU
![alt text](image2.png)
3. 准备三个终端窗口进行协同调试
终端1：运行make debug启动QEMU
在项目目录下执行：
```
make debug
```
这个命令会启动我们新编译的调试版QEMU，并暂停在初始状态。

终端2：使用普通GDB附加到QEMU进程，调试QEMU源码
首先，我们需要找到QEMU进程的PID：
```
pgrep -f qemu-system-riscv64
```
得到PID：13220
然后启动GDB并附加到这个进程：
```
sudo gdb
(gdb)attach 13220
(gdb) handle SIGPIPE nostop noprint
(gdb) # 你可以在这里执行一些操作，设置一些断点等
(gdb) continue # 之后就启动执行
```
关于handle SIGPIPE nostop noprint
SIGPIPE 是一种 Unix 信号，当进程试图向一个已经被关闭的管道或套接字写入数据时会产生这个信号。在 QEMU 调试过程中，这种情况可能会频繁发生。
这个命令告诉 GDB：
* nostop - 收到 SIGPIPE 时不暂停程序执行
* noprint - 收到 SIGPIPE 时不打印相关信息
如果不处理 SIGPIPE：
（1）GDB 会中断执行
当 QEMU 进程收到 SIGPIPE 信号时，GDB 默认会暂停程序执行
你会看到类似这样的信息：Program received signal SIGPIPE, Broken pipe.

（2）调试过程被打断
每次出现 SIGPIPE 信号，GDB 都会停止并等待你的输入
你需要手动输入 continue 或 signal 13 等命令来继续执行
这会使调试过程变得非常繁琐和低效

（3）影响调试体验
SIGPIPE 信号在 QEMU 正常运行过程中可能会频繁出现
频繁的手动干预会影响你对真正关注的断点的调试

之后执行的操作：
```
(gdb) break riscv_cpu_tlb_fill
(gdb) break riscv_cpu_exec_interrupt
(gdb) break get_physical_address
```
这里设置了三个断点当QEMU执行到这些关键代码时会自动暂停，方便观察地址翻译的具体过程。

终端3：使用riscv64-unknown-elf-gdb调试运行在QEMU中的ucore内核
在gdb已经设置好断点并启动qemu的执行（即执行了continue）之后，在项目目录下执行：
```
make gdb
```
这个GDB会话会连接到QEMU的GDB stub，就像我们平时调试内核一样。可以在这里控制ucore的执行，比如打断点让它在某个访存指令处暂停、通过指令查看寄存器的值，某个符号的地址等。

4. 观察地址转换流程
 make gdb之后，可以看到gdb的内容：
```

 Thread 1 "qemu-system-ris" hit Breakpoint 4, get_physical_address (
    env=0x5df792530140, physical=0x7ffc74b23208, prot=0x7ffc74b23200, addr=0, 
    access_type=0, mmu_idx=3)
    at /home/liuyueshuai/桌面/labcode/lab1/qemu-4.1.1/target/riscv/cpu_helper.c:158
158	{
```
这里断点在get_physical_address函数中，这个函数是QEMU中用于获取物理地址的函数。
* 函数：get_physical_address
* 虚拟地址：addr=0
* 访问类型：access_type=0（读访问）
* MMU索引：mmu_idx=3
下面执行list命令查看get_physical_address函数的源码：
```
此处仅展示部分
{
    /* NOTE: the env->pc value visible here will not be
     * correct, but the value visible to the exception handler
     * (riscv_cpu_do_interrupt) is correct */

    int mode = mmu_idx;

    if (mode == PRV_M && access_type != MMU_INST_FETCH) {
        if (get_field(env->mstatus, MSTATUS_MPRV)) {
            mode = get_field(env->mstatus, MSTATUS_MPP);
        }
    }

    if (mode == PRV_M || !riscv_feature(env, RISCV_FEATURE_MMU)) {
        *physical = addr;
        *prot = PAGE_READ | PAGE_WRITE | PAGE_EXEC;
        return TRANSLATE_SUCCESS;
    }

    *prot = 0;

    target_ulong base;
    int levels, ptidxbits, ptesize, vm, sum;
    int mxr = get_field(env->mstatus, MSTATUS_MXR);

    if (env->priv_ver >= PRIV_VERSION_1_10_0) {
        base = get_field(env->satp, SATP_PPN) << PGSHIFT;
        sum = get_field(env->mstatus, MSTATUS_SUM);
        vm = get_field(env->satp, SATP_MODE);
        switch (vm) {
        case VM_1_10_SV32:
          levels = 2; ptidxbits = 10; ptesize = 4; break;
        case VM_1_10_SV39:
          levels = 3; ptidxbits = 9; ptesize = 8; break;
        case VM_1_10_SV48:
          levels = 4; ptidxbits = 9; ptesize = 8; break;
        case VM_1_10_SV57:
          levels = 5; ptidxbits = 9; ptesize = 8; break;
        case VM_1_10_MBARE:
            *physical = addr;
            *prot = PAGE_READ | PAGE_WRITE | PAGE_EXEC;
            return TRANSLATE_SUCCESS;
        default:
          g_assert_not_reached();
        }
    } else {
        base = env->sptbr << PGSHIFT;
        sum = !get_field(env->mstatus, MSTATUS_PUM);
        vm = get_field(env->mstatus, MSTATUS_VM);
        switch (vm) {
        case VM_1_09_SV32:
          levels = 2; ptidxbits = 10; ptesize = 4; break;
        case VM_1_09_SV39:
          levels = 3; ptidxbits = 9; ptesize = 8; break;
        case VM_1_09_SV48:
          levels = 4; ptidxbits = 9; ptesize = 8; break;
        case VM_1_09_MBARE:
            *physical = addr;
            *prot = PAGE_READ | PAGE_WRITE | PAGE_EXEC;
            return TRANSLATE_SUCCESS;
        default:
          g_assert_not_reached();
        }
    }

    CPUState *cs = env_cpu(env);
    int va_bits = PGSHIFT + levels * ptidxbits;
    target_ulong mask = (1L << (TARGET_LONG_BITS - (va_bits - 1))) - 1;
    target_ulong masked_msbs = (addr >> (va_bits - 1)) & mask;
    if (masked_msbs != 0 && masked_msbs != mask) {
        return TRANSLATE_FAIL;
    }

    int ptshift = (levels - 1) * ptidxbits;
    int i;
    ......
}
```
1.**地址翻译入口点**
当CPU需要访问一个虚拟地址时，QEMU会调用get_physical_address()函数来执行地址翻译。这个函数接收以下参数：

* CPURISCVState *env: CPU状态，包含寄存器和其他相关信息
* hwaddr *physical: 输出参数，用于存储翻译后的物理地址
* int *prot: 输出参数，用于存储页面的访问权限
* target_ulong addr: 需要翻译的虚拟地址
* int access_type: 访问类型（指令获取、数据加载或数据存储）
* int mmu_idx: MMU索引，表示当前特权级别

2.**确定页表基址和模式**
首先，函数会根据当前CPU的特权级别确定使用的页表：

```c
if (mode == PRV_M) {
    // 机器模式(M-mode)通常直接访问物理地址，除非显式启用分页
    // ...
} else {
    // 用户模式(U-mode)或监管模式(S-mode)使用页表
    base = env->sptbr << PGSHIFT;  // 在较老版本中使用sptbr
    // 或者在新版本中使用:
    // base = get_field(env->satp, SATP_PPN) << PGSHIFT;
    
    vm = get_field(env->mstatus, MSTATUS_VM);  // 或使用SATP中的模式位
    switch (vm) {
    case VM_1_09_SV32:
      levels = 2; ptidxbits = 10; ptesize = 4; break;
    case VM_1_09_SV39:
      levels = 3; ptidxbits = 9; ptesize = 8; break;
    case VM_1_09_SV48:
      levels = 4; ptidxbits = 9; ptesize = 8; break;
    // ...
    }
}
```
3. 逐级页表遍历
接下来，函数会逐级遍历页表：

```c
int ptshift = (levels - 1) * ptidxbits;
for (i = 0; i < levels; i++, ptshift -= ptidxbits) {
    // 计算当前级页表的索引
    target_ulong idx = (addr >> (PGSHIFT + ptshift)) & ((1 << ptidxbits) - 1);
    
    // 计算页表项的物理地址
    target_ulong pte_addr = base + idx * ptesize;
    
    // 从物理内存中读取页表项
#if defined(TARGET_RISCV32)
    target_ulong pte = ldl_phys(cs->as, pte_addr);
#elif defined(TARGET_RISCV64)
    target_ulong pte = ldq_phys(cs->as, pte_addr);
#endif
    
    // 解析页表项中的物理页号
    target_ulong ppn = pte >> PTE_PPN_SHIFT;
```
然后使用step命令进行逐行调试，查看get_physical_address函数的运行过程，重点查看TLB查询的过程，使用print env->satp可以观察到satp的值。
这个函数本身是TLB缺失时的处理函数。当硬件TLB未命中时，会调用此函数进行软件页表遍历。
在函数末尾，计算出的物理地址和权限信息会被用于更新TLB条目：
```
*physical = (ppn | (vpn & ((1L << ptshift) - 1))) << PGSHIFT;
```


